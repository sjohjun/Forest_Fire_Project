# -*- coding:utf-8 -*-
import streamlit as st
import pandas as pd
import numpy as np
import geopandas as gpd
import pandas_gbq

import requests
from bs4 import BeautifulSoup
import json
import lxml

from datetime import datetime, timedelta
import time

import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import folium
from streamlit_folium import st_folium
from folium.plugins import MarkerCluster
import branca.colormap as cm

from shapely.geometry import Point, Polygon, MultiPolygon, shape
from shapely import wkt

import statsmodels.api as sm

from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.model_selection import KFold, StratifiedKFold, TimeSeriesSplit, train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score, cross_val_predict
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, confusion_matrix, classification_report,  roc_curve, auc, RocCurveDisplay
from imblearn.over_sampling import SMOTE

from xgboost import XGBClassifier, plot_importance
from lightgbm import LGBMClassifier, plot_importance

import googlemaps
from google.cloud import bigquery
from google.oauth2 import service_account

import utils
import eda_app
import stat_app
import model_app
import service_app

from utils import credentials, servicekey

import os
import warnings
warnings.filterwarnings("ignore")

def get_weather_days_data(serviceKey, weather_stations, start_date_str=None, end_date_str=None):
    """
    ì§€ì •í•œ ê¸°ìƒ ê´€ì¸¡ì†Œì˜ ì¼ë³„ ë‚ ì”¨ ë°ì´í„°ë¥¼ ì¡°íšŒí•˜ì—¬ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.

    Args:
        serviceKey (str): ê³µê³µë°ì´í„°í¬í„¸ì—ì„œ ë°œê¸‰ë°›ì€ ì¸ì¦í‚¤.
        weather_stations (pandas.DataFrame): ê¸°ìƒ ê´€ì¸¡ì†Œ ì •ë³´ê°€ í¬í•¨ëœ ë°ì´í„°í”„ë ˆì„.
        start_date_str (str, optional): ì¡°íšŒ ì‹œì‘ ë‚ ì§œë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë¬¸ìì—´ (ì˜ˆ: "20220101").
            ê¸°ë³¸ê°’ì€ Noneì´ë©°, ê¸°ë³¸ê°’ì¼ ê²½ìš° 2013ë…„ 1ì›” 1ì¼ë¡œ ì„¤ì •ë©ë‹ˆë‹¤.
        end_date_str (str, optional): ì¡°íšŒ ë ë‚ ì§œë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë¬¸ìì—´ (ì˜ˆ: "20220331").
            ê¸°ë³¸ê°’ì€ Noneì´ë©°, ê¸°ë³¸ê°’ì¼ ê²½ìš° ì–´ì œ ë‚ ì§œë¡œ ì„¤ì •ë©ë‹ˆë‹¤.

    Returns:
        pandas.DataFrame: ì¡°íšŒëœ ì¼ë³„ ë‚ ì”¨ ë°ì´í„°ë¥¼ ë‹´ì€ ë°ì´í„°í”„ë ˆì„ ê°ì²´.
    """

    url = 'http://apis.data.go.kr/1360000/AsosDalyInfoService/getWthrDataList'

    # ì‹œì‘ ë‚ ì§œì™€ ë ë‚ ì§œë¥¼ ìƒì„±í•©ë‹ˆë‹¤
    if start_date_str is None:
        start_date = datetime.now() - timedelta(days=9)  # ì‹œì‘ ë‚ ì§œë¥¼ 2013ë…„ 1ì›” 1ì¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤
    else:
        start_date = datetime.strptime(start_date_str, "%Y%m%d")

    if end_date_str is None:
        end_date = datetime.now() - timedelta(days=2)  # ì–´ì œ ë‚ ì§œë¥¼ êµ¬í•˜ê¸° ìœ„í•´ í˜„ì¬ ë‚ ì§œì—ì„œ 1ì¼ì„ ëºë‹ˆë‹¤
    else:
        end_date = datetime.strptime(end_date_str, "%Y%m%d")

    end_date_str = end_date.strftime("%Y%m%d")

    all_data = []  # ì „ì²´ ë°ì´í„°ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤

    for stnNm in weather_stations["stnId"]:
        params = {
            'serviceKey': serviceKey,
            'pageNo': '1',  # ì´ˆê¸° í˜ì´ì§€ ë²ˆí˜¸ë¥¼ 1ë¡œ ì„¤ì •í•©ë‹ˆë‹¤
            'numOfRows': '999',  # í•œ í˜ì´ì§€ì— ìµœëŒ€ë¡œ ê°€ì ¸ì˜¬ ë°ì´í„° ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤
            'dataType': 'json',
            'dataCd': 'ASOS',
            'dateCd': 'DAY',
            'startDt': start_date.strftime("%Y%m%d"),  # ì‹œì‘ ë‚ ì§œë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ì„¤ì •í•©ë‹ˆë‹¤
            'endDt': end_date_str,  # ë ë‚ ì§œë¥¼ ì–´ì œ ë‚ ì§œë¡œ ì„¤ì •í•©ë‹ˆë‹¤
            'stnIds': stnNm
        }

        while True:
            try:
                response = requests.get(url, params=params)
                response.raise_for_status()  # ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´ ì˜ˆì™¸ë¥¼ ë°œìƒì‹œí‚´
                data = response.json()
                all_data.extend(data['response']['body']['items']['item'])

                # ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™
                params['pageNo'] = str(int(params['pageNo']) + 1)
                if int(params['pageNo']) > int(
                        int(data['response']['body']['totalCount']) / int(params['numOfRows'])) + 1:
                    break
            except requests.exceptions.HTTPError as e:
                print("API ìš”ì²­ ì˜¤ë¥˜:", e.response.text)  # API ìš”ì²­ ì˜¤ë¥˜ ë©”ì‹œì§€ ì¶œë ¥
                break
            except Exception as e:
                print(params)
                print(response.content)
                print("ì˜ˆì™¸ ë°œìƒ:", e)  # ê¸°íƒ€ ì˜ˆì™¸ ë°œìƒ ì‹œ ë©”ì‹œì§€ ì¶œë ¥
                break

    # ë¦¬ìŠ¤íŠ¸ì—ì„œ ë°ì´í„°í”„ë ˆì„ì„ ìƒì„±í•©ë‹ˆë‹¤
    weather_days = pd.DataFrame(all_data)

    return weather_days

def get_dataframe_from_bigquery(dataset_id, table_id):
    """
    ì£¼ì–´ì§„ BigQuery í…Œì´ë¸”ì—ì„œ ë°ì´í„°ë¥¼ ì¡°íšŒí•˜ì—¬ DataFrameìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.

    Args:
        dataset_id (str): ëŒ€ìƒ ë°ì´í„°ì…‹ì˜ ID.
        table_id (str): ëŒ€ìƒ í…Œì´ë¸”ì˜ ID.
        key_path (str): ì„œë¹„ìŠ¤ ê³„ì • í‚¤ íŒŒì¼ì˜ ê²½ë¡œ.

    Returns:
        pandas.DataFrame: ì¡°íšŒëœ ë°ì´í„°ë¥¼ ë‹´ì€ DataFrame ê°ì²´.
    """

    # BigQuery í´ë¼ì´ì–¸íŠ¸ ìƒì„±
    client = bigquery.Client(credentials=credentials, project=credentials.project_id)

    # í…Œì´ë¸” ë ˆí¼ëŸ°ìŠ¤ ìƒì„±
    table_ref = client.dataset(dataset_id).table(table_id)

    # í…Œì´ë¸” ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
    df = client.list_rows(table_ref).to_dataframe()

    return df


def get_geodataframe_from_bigquery(dataset_id, table_id):
    """
    ì£¼ì–´ì§„ BigQuery í…Œì´ë¸”ì—ì„œ ë°ì´í„°ë¥¼ ì¡°íšŒí•˜ì—¬ Geopandas GeoDataFrameìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.

    Args:
        dataset_id (str): ëŒ€ìƒ ë°ì´í„°ì…‹ì˜ ID.
        table_id (str): ëŒ€ìƒ í…Œì´ë¸”ì˜ ID.
        key_path (str): ì„œë¹„ìŠ¤ ê³„ì • í‚¤ íŒŒì¼ì˜ ê²½ë¡œ.

    Returns:
        geopandas.GeoDataFrame: ì¡°íšŒëœ ë°ì´í„°ë¥¼ ë‹´ì€ Geopandas GeoDataFrame ê°ì²´.
    """

    # ë¹…ì¿¼ë¦¬ í´ë¼ì´ì–¸íŠ¸ ê°ì²´ ìƒì„±
    client = bigquery.Client(credentials=credentials)

    # ì¿¼ë¦¬ ì‘ì„±
    query = f"SELECT * FROM `{dataset_id}.{table_id}`"

    # ì¿¼ë¦¬ ì‹¤í–‰
    df = client.query(query).to_dataframe()

    # 'geometry' ì—´ì˜ ë¬¸ìì—´ì„ ë‹¤ê°í˜• ê°ì²´ë¡œ ë³€í™˜
    df['geometry'] = df['geometry'].apply(wkt.loads)

    # GeoDataFrameìœ¼ë¡œ ë³€í™˜
    gdf = gpd.GeoDataFrame(df, geometry='geometry')
    gdf.crs = "EPSG:4326"

    return gdf

def today_weather(weather_stations):
    """
    ë‚ ì”¨ ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.

    Args:
        - weather_stations (pandas.DataFrame): ê¸°ìƒ ê´€ì¸¡ì†Œ ì •ë³´ê°€ í¬í•¨ëœ ë°ì´í„°í”„ë ˆì„

    Returns:
        pandas.DataFrame: ì „ì²˜ë¦¬ëœ ë‚ ì”¨ ë°ì´í„°ê°€ í¬í•¨ëœ ë°ì´í„°í”„ë ˆì„
    """
    # ë‚ ì”¨ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    weather_days = get_weather_days_data(servicekey, weather_stations)

    # í•„ìš”ì—†ëŠ” ì—´ ì œê±°
    weather_days = weather_days.drop(['stnNm', 'minTaHrmt', 'maxTaHrmt', 'mi10MaxRn', 'mi10MaxRnHrmt', 'hr1MaxRn', 'hr1MaxRnHrmt',
                       'sumRnDur', 'hr24SumRws', 'maxWd', 'avgTd', 'avgPv', 'avgPa', 'maxPs', 'maxPsHrmt',
                       'minPs', 'minPsHrmt', 'avgPs', 'ssDur', 'sumSsHr', 'hr1MaxIcsrHrmt', 'hr1MaxIcsr',
                       'sumGsr', 'ddMefs', 'ddMefsHrmt', 'ddMes', 'ddMesHrmt', 'sumDpthFhsc', 'avgTs', 'minTg',
                       'avgCm5Te', 'avgCm10Te', 'avgCm20Te', 'avgCm30Te', 'avgM05Te', 'avgM10Te', 'avgM15Te',
                       'avgM30Te', 'avgM50Te', 'sumLrgEv', 'sumSmlEv', 'n99Rn', 'iscs', 'sumFogDur',
                       'maxInsWsWd', 'maxInsWsHrmt', 'maxWsWd', 'maxWsHrmt', 'minRhmHrmt', 'avgTca', 'avgLmac'], axis=1)

    # ë‚ ì§œ ë°ì´í„° íƒ€ì…ìœ¼ë¡œ ë³€í™˜
    weather_days['tm'] = pd.to_datetime(weather_days['tm'], errors='coerce')

    # ìˆ«ìë¡œ ë³€í™˜í•  ì—´ ì„ íƒ
    numeric_columns = weather_days.columns.drop("tm")

    # ìˆ«ìë¡œ ë³€í™˜
    weather_days[numeric_columns] = weather_days[numeric_columns].apply(pd.to_numeric, errors='coerce')

    # ê²°ì¸¡ê°’ 0ìœ¼ë¡œ ì±„ìš°ê¸°
    weather_days['sumRn'].fillna(0, inplace=True)

    # stnId ë³„ë¡œ ë°ì´í„°í”„ë ˆì„ ë¶„í• 
    dfs = []
    for stn_id, group in weather_days.groupby("stnId"):
        # Shiftëœ ì—´ì— ì²˜ìŒ ê°’ì„ ì¶”ê°€
        group["h1"] = group["avgRhm"].shift(1)
        group.loc[group.index[0], "h1"] = group["avgRhm"].iloc[0]

        group["h2"] = group["h1"].shift(1)
        group.loc[group.index[0], "h2"] = group["avgRhm"].iloc[0]

        group["h3"] = group["h2"].shift(1)
        group.loc[group.index[0], "h3"] = group["avgRhm"].iloc[0]

        group["h4"] = group["h3"].shift(1)
        group.loc[group.index[0], "h4"] = group["avgRhm"].iloc[0]

        # ì‹¤íš¨ìŠµë„ ê³„ì‚°
        r = 0.7
        group["effRhm"] = ((group["avgRhm"]) + (r**1)*(group["h1"]) + (r**2)*(group["h2"]) + (r**3)*(group["h3"]) + (r**4)*(group["h4"])) * (1-r)

        # 6ì¼ì „ë¶€í„° ê¸°ì¤€ì¼ê¹Œì§€ 7ì¼ê°„ ê°•ìˆ˜ëŸ‰(mm)
        window_size = 7
        group['sumRn7'] = group['sumRn'].rolling(window_size, min_periods=1).sum()

        # ê°•ìˆ˜ ì—¬ë¶€, ë¹„ ì˜´ 1 / ë¹„ ì•ˆì˜´ 0
        group['Rntf'] = group['sumRn'].apply(lambda x: 1 if x > 0 else 0)

        # 6ì¼ì „ë¶€í„° ê¸°ì¤€ì¼ê¹Œì§€ 7ì¼ê°„ ìµœëŒ€í’ì†
        group['maxwind7'] = group['maxWs'].rolling(window_size, min_periods=1).max()

        # ë¹„ê°€ ì˜¤ì§€ ì•Šì€ ë‚ ì˜ ì¼ìˆ˜ë¥¼ ì €ì¥í•  ìƒˆë¡œìš´ ì¹¼ëŸ¼ ì¶”ê°€
        group['noRn'] = 0

        # ì¼ê°•ìˆ˜ëŸ‰ì´ 0ì¸ ë‚ ì˜ ì—°ì†ëœ ì¼ìˆ˜ë¥¼ ê³„ì‚°í•˜ì—¬ noRn ì¹¼ëŸ¼ì— ì €ì¥
        count = 0
        for i, value in enumerate(group['sumRn']):
            if value == 0:
                count += 1
            else:
                group.loc[group.index[i], 'noRn'] = count
                count = 0

        dfs.append(group)

    # ë°ì´í„°í”„ë ˆì„ í•©ì¹˜ê¸°
    weather_days = pd.concat(dfs)

    # ê¸°ìƒ ê´€ì¸¡ì†Œ ì •ë³´ì™€ ë³‘í•©
    weather_days = weather_days.merge(weather_stations, on='stnId')

    # í•„ìš”ì—†ëŠ” ì—´ ì œê±°
    weather_days = weather_days.drop(['stnId', 'stnAddress', 'stnLatitude', 'stnLongitude', 'h1', 'h2', 'h3', 'h4'], axis=1)

    # ë‚ ì§œ ì„¤ì •
    target_date = (datetime.now() - timedelta(days=2)).strftime("%Y-%m-%d")

    # í•„ìš”í•œ ì—´ì„ ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹í™”í•˜ê³  í‰ê·  ê³„ì‚°
    weather_days = weather_days[weather_days["tm"] == target_date].reset_index(drop=True)
    weather_days = weather_days.groupby(["w_regions", "tm"]).agg({
        "avgTa": "mean",
        "minTa": "mean",
        "maxTa": "mean",
        "sumRn": "mean",
        "maxInsWs": "mean",
        "maxWs": "mean",
        "avgWs": "mean",
        "minRhm": "mean",
        "avgRhm": "mean",
        "effRhm": "mean",
        "sumRn7": "mean",
        "Rntf": lambda x: int(np.any(x == 1)),
        "maxwind7": "mean",
        "noRn": "mean",
    }).reset_index()

    # ì†Œìˆ˜ì  ìë¦¬ìˆ˜ ì„¤ì •
    weather_days = weather_days.round({"avgTa": 2, "minTa": 2, "maxTa": 2, "sumRn": 2, "maxInsWs": 2, "maxWs": 2, "avgWs": 2, "minRhm": 2, "avgRhm": 2, "effRhm": 2, "sumRn7": 2})

    # í•„ìš”ì—†ëŠ” ì—´ ì œê±°
    weather_days = weather_days.drop(['tm'], axis=1)

    return weather_days


def split_train_test(data):
    """
    ì…ë ¥ëœ ë°ì´í„°ë¥¼ í•™ìŠµ ë° í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ë¶„í• í•˜ê³  í´ë˜ìŠ¤ ë¶ˆê· í˜•ì„ í•´ê²°í•˜ê¸° ìœ„í•´ SMOTEë¥¼ ì ìš©í•©ë‹ˆë‹¤.

    Args:
        data (DataFrame): í”¼ì²˜ì™€ ë ˆì´ë¸”ì„ í¬í•¨í•˜ëŠ” ì…ë ¥ ë°ì´í„°.

    Returns:
        tuple: 4ê°œì˜ ìš”ì†Œë¥¼ í¬í•¨í•˜ëŠ” íŠœí”Œ:
            - X_train_over (DataFrame): SMOTEë¥¼ ì ìš©í•œ í•™ìŠµìš© í”¼ì²˜ ë°ì´í„°.
            - X_test (DataFrame): í…ŒìŠ¤íŠ¸ìš© í”¼ì²˜ ë°ì´í„°.
            - y_train_over (Series): SMOTEë¥¼ ì ìš©í•œ í•™ìŠµìš© ë ˆì´ë¸” ë°ì´í„°.
            - y_test (Series): í…ŒìŠ¤íŠ¸ìš© ë ˆì´ë¸” ë°ì´í„°.
    """

    # ê¸°ê°„ì„ ê³ ë ¤í•˜ì—¬ train, test ë°ì´í„° ë‚˜ëˆ„ê¸°
    train_start = '2013-01-01'
    train_end = '2020-12-31'
    test_start = '2021-01-01'
    test_end = '2022-12-31'

    train_mask = (data['tm'] >= train_start) & (data['tm'] <= train_end)
    test_mask = (data['tm'] >= test_start) & (data['tm'] <= test_end)

    train_data = data[train_mask]
    test_data = data[test_mask]

    X_train = train_data.drop(['w_regions', 'tm', 'fire_occur'], axis=1)
    y_train = train_data['fire_occur']

    X_train = X_train.astype(float)
    y_train = y_train.astype(int)

    # SMOTE ì ìš©
    smote = SMOTE(random_state=42)

    X_train_over, y_train_over = smote.fit_resample(X_train, y_train)

    X_test = test_data.drop(['w_regions', 'tm', 'fire_occur'], axis=1)
    y_test = test_data['fire_occur']

    X_test = X_test.astype(float)
    y_test = y_test.astype(int)

    return X_train_over, X_test, y_train_over, y_test

def train_logistic_regression(X_train, y_train):
    # Train logistic regression model
    lr_model = LogisticRegression(solver='liblinear', random_state=0)
    lr_model.fit(X_train, y_train)
    return lr_model

def train_xgboost(X_train, y_train):
    # Train XGBoost model
    params = {
        'objective': 'binary:logistic',
        'max_depth': 4,
        'alpha': 10,
        'learning_rate': 1.0,
        'n_estimators': 100
    }
    xgb_model = XGBClassifier(booster='gbtree', importance_type='gain', **params)
    xgb_model.fit(X_train, y_train)
    return xgb_model

def train_lightgbm(X_train, y_train):
    # Train LightGBM model
    params = {
        'class_weight': 'balanced',
        'drop_rate': 0.9,
        'min_data_in_leaf': 100,
        'max_bin': 255,
        'n_estimators': 500,
        'min_sum_hessian_in_leaf': 1,
        'learning_rate': 0.1,
        'bagging_fraction': 0.85,
        'colsample_bytree': 1.0,
        'feature_fraction': 0.1,
        'lambda_l1': 5.0,
        'lambda_l2': 3.0,
        'max_depth': 9,
        'min_child_samples': 55,
        'min_child_weight': 5.0,
        'min_split_gain': 0.1,
        'num_leaves': 45,
        'subsample': 0.75
    }
    lgb_model = LGBMClassifier(boosting_type='dart', importance_type='gain', **params)
    lgb_model.fit(X_train, y_train)
    return lgb_model

def get_dwi_by_pred(pred_proba, num_intervals=10):
    """
    ì˜ˆì¸¡ê°’ì„ ê¸°ë°˜ìœ¼ë¡œ DWI(Drought Warning Index) ê°’ì„ ê³„ì‚°í•˜ì—¬ ì¶œë ¥í•©ë‹ˆë‹¤.

    Args:
        pred_proba (array-like): ì˜ˆì¸¡ê°’.
        num_intervals (int, optional): DWI ë“±ê¸‰ì˜ ê°œìˆ˜. ê¸°ë³¸ê°’ì€ 10ì…ë‹ˆë‹¤.

    Returns:
        int: DWI ê°’
    """

    interval_idx = int(pred_proba * num_intervals)
    if interval_idx == num_intervals:
        interval_idx -= 1
    dwi = interval_idx + 1
    return dwi

def create_dwi_choropleth_map(dataframe, geometry_column, dwi_columns):
    """
    GeoDataFrameì„ ê¸°ë°˜ìœ¼ë¡œ DWI ë“±ê¸‰ Choropleth ë§µì„ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        dataframe (geopandas.GeoDataFrame): Choropleth ë§µì„ ìƒì„±í•  GeoDataFrame.
        geometry_column (str): ì§€ì˜¤ë©”íŠ¸ë¦¬ ì •ë³´ë¥¼ í¬í•¨í•˜ëŠ” ì—´ì˜ ì´ë¦„.
        dwi_columns (str): DWI ë“±ê¸‰ ê°’ì„ í¬í•¨í•˜ëŠ” ì—´ì˜ ì´ë¦„.

    Returns:
        folium.Map: ìƒì„±ëœ Choropleth ë§µ ê°ì²´.
    """
    # í‘œí˜„í•  ì¢Œí‘œê³„ ì„¤ì •
    dataframe.crs = "EPSG:4326"

    # ì§€ë„ ìƒì„±
    map = folium.Map(location=[37.7, 128.3], zoom_start=8)

    # DWI ë“±ê¸‰ ìˆ˜ì™€ ë²”ìœ„ ì„¤ì •
    num_intervals = 10
    min_value = 1
    max_value = 10
    interval_size = (max_value - min_value) / num_intervals

    # ë“±ê¸‰ ìƒ‰ìƒ ë§µ ì„¤ì •
    colormap = cm.linear.YlOrRd_09.scale(min_value, max_value)

    # í…Œë‘ë¦¬ ì„  ìŠ¤íƒ€ì¼ í•¨ìˆ˜
    def style_function(feature):
        dwi_value = feature['properties'][dwi_columns]
        color = colormap(dwi_value)
        return {
            'fillColor': color,
            'fillOpacity': 0.7,
            'color': 'black',
            'weight': 1,
            'dashArray': '5, 5'
        }

    # Choropleth ë§µ ìƒì„±
    folium.GeoJson(
        dataframe,
        name='choropleth',
        style_function=style_function,
        tooltip=folium.GeoJsonTooltip(fields=[dwi_columns], labels=True, sticky=False),
        highlight_function=lambda x: {'weight': 3},
    ).add_to(map)

    # ë²”ë¡€ ì¶”ê°€
    colormap.add_to(map)
    map.add_child(colormap)

    st_folium(map)

def home_app():
    """
        Renders the introduction section of the app, including tabs for overview, objectives, and analysis phases.
    """
    weather_stations = get_dataframe_from_bigquery("PREPROCESSING_DATA", "weather_stations").sort_values(["stnId"])
    gangwon_regions = get_geodataframe_from_bigquery("PREPROCESSING_DATA", "gangwon_regions")

    weather_days = today_weather(weather_stations)

    model_data = {
        "ê°•ì›ë¶ë¶€ë‚´ë¥™": ("GangwonNorthInland", train_lightgbm),
        "ê°•ì›ë¶ë¶€ì‚°ì§€": ("GangwonNorthMount", train_logistic_regression),
        "ê°•ì›ë¶ë¶€í•´ì•ˆ": ("GangwonNorthCoast", train_logistic_regression),
        "ê°•ì›ì¤‘ë¶€ë‚´ë¥™": ("GangwonCentralInland", train_logistic_regression),
        "ê°•ì›ì¤‘ë¶€ì‚°ì§€": ("GangwonCentralMount", train_lightgbm),
        "ê°•ì›ì¤‘ë¶€í•´ì•ˆ": ("GangwonCentralCoast", train_logistic_regression),
        "ê°•ì›ë‚¨ë¶€ë‚´ë¥™": ("GangwonSouthInland", train_logistic_regression),
        "ê°•ì›ë‚¨ë¶€ì‚°ì§€": ("GangwonSouthMount", train_logistic_regression),
        "ê°•ì›ë‚¨ë¶€í•´ì•ˆ": ("GangwonSouthInland", train_logistic_regression)
    }

    dwi_data = []
    for region, (data_table, model_func) in model_data.items():
        data = get_dataframe_from_bigquery("ANALSIS_DATA", data_table).sort_values(["tm"]).reset_index(drop=True)
        data = data[data['tm'] < '2023-01-01']
        X_train, X_test, y_train, y_test = split_train_test(data)
        model = model_func(X_train, y_train)
        pred_proba = model.predict_proba(weather_days[weather_days["w_regions"] == region].drop(['w_regions'], axis=1))[:, 1]
        dwi = get_dwi_by_pred(pred_proba)
        dwi_data.append((region, dwi))

    dwi_df = pd.DataFrame(dwi_data, columns=['w_regions', 'DWI'])
    merged_df = gangwon_regions.merge(dwi_df, on='w_regions', how='left')
    target_date = (datetime.now() - timedelta(days=2)).strftime("%Y-%m-%d")


    empyt1, con1, empty2 = st.columns([0.2, 1.0, 0.2])
    empyt1, con2, empty2 = st.columns([0.4, 1.0, 0.4])
    empyt1, con3, empty2 = st.columns([0.5, 0.5, 0.5])
    empyt1, con4, empty2 = st.columns([0.4, 1.0, 0.4])

    with con1:
        st.markdown(
            "<h2 style='text-align: center; color: black;'>ê°•ì›ë„ ì‚°ë¶ˆ ì˜ˆì¸¡ ë° í”¼í•´ ìµœì†Œí™” í”„ë¡œì íŠ¸</span>", unsafe_allow_html=True)
        st.write('<hr>', unsafe_allow_html=True)
    with con2:
        st.markdown("<h4 style='font-size: 24px; text-align: center; color: black;'>ğŸ”¥ğŸŒ³ ì‹¤ì‹œê°„ ì‚°ë¶ˆìœ„í—˜ì§€ìˆ˜(DWI) ğŸŒ³ğŸ”¥</h4>",
                    unsafe_allow_html=True)
        st.markdown("")
        st.markdown(f"<h6 style='font-size: 16px; text-align: center; color: black;'> ({target_date}) ì‹¤ì‹œê°„ DWI ì§€ìˆ˜ ì‹œê°í™” </h4>",
                    unsafe_allow_html=True)
    with con3:
        create_dwi_choropleth_map(merged_df, "geometry", "DWI")
    with con4:
        st.markdown(
            "<h6 style='font-size: 16px; text-align: center; color: black;'> ê°•ì›ë„ 9ê°œ ì§€ì—­ë³„ ì„¤ì •í•œ ML MODEL ì— ì…ë ¥í•˜ì—¬, </h4>",
            unsafe_allow_html=True)
        st.markdown("")
        st.markdown(
            "<h6 style='font-size: 16px; text-align: center; color: black;'> ì–»ì–´ì§„ í™•ë¥ ë“¤ì˜ ì˜ˆì¸¡ì¹˜ë¥¼ ì´ìš©í•˜ì—¬ ì‚°ë¶ˆìœ„í—˜ì§€ìˆ˜(DWI) ì§€ë„ì‹œê°í™” </h4>",
            unsafe_allow_html=True)

    st.write('<hr>', unsafe_allow_html=True)

    # Link
    c1, c2, c3 = st.columns(3)
    with c1:
        st.info('**Project: [@KingBeeM/ForestFire](https://github.com/KingBeeM/ForestFire)**', icon="ğŸ’¡")
    with c2:
        st.info('**GitHub: [@KingBeeM](https://github.com/KingBeeM)**', icon="ğŸ’»")
    with c3:
        st.info('**Data: [Public API](https://www.data.go.kr/data/15059093/openapi.do)**', icon="ğŸ“•")